{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "from scipy.stats import multivariate_normal\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chinese_restaurant_process_clustering(alpha,seed,k_,num_cluster):\n",
    "    print(\"\\nComputing KMeans with KMeans++ Initialisation....\\n\")\n",
    "    km = KMeans(n_clusters=k_,max_iter=20)\n",
    "    print(\"Computed KMeans with initialisation!\\n\")\n",
    "    \"\"\"\n",
    "        Here we are adding the dataset - since we are going to be performing gaussian\n",
    "        mixture model clustering using dirichlet processes, we are going to sample the\n",
    "        data points from a mixture of gaussians - just for this demo, we are setting cluster \n",
    "        sizes arbitrarily\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    dataset_1 = np.random.multivariate_normal([5, 5], np.diag([0.5, 0.5]), size=10)\n",
    "    dataset_2 = np.random.multivariate_normal([7.5, 8], np.diag([0.5, 0.5]), size=60)\n",
    "    dataset_3 = np.random.multivariate_normal([10, 12], np.diag([0.5, 0.5]), size=40)\n",
    "    dataset_4 = np.random.multivariate_normal([6,11],np.diag([1,1]),size=50)\n",
    "    dataset_5 = np.random.multivariate_normal([9,11],np.diag([1,1]),size=60)\n",
    "    dataset_6 = np.random.multivariate_normal([10,6],np.diag([0.5,0.5]),size=50)\n",
    "    \n",
    "    dataset_total = np.vstack([dataset_1,dataset_2,dataset_3,dataset_4,dataset_5,dataset_6])\n",
    "    \n",
    "    if num_cluster<=6:\n",
    "        arr = [10,70,110,160,220,270]\n",
    "        dataset_total = dataset_total[:arr[num_cluster-1]]\n",
    "    else:\n",
    "        temp = num_cluster-6\n",
    "        new_cluster = np.random.multivariate_normal([10,9],np.diag([0.75, 0.75]), size=50)\n",
    "        dataset_total = np.vstack([dataset_total,new_cluster])\n",
    "        new_cluster = np.random.multivariate_normal([0,6],np.diag([0.75, 0.75]), size=50)\n",
    "        dataset_total = np.vstack([dataset_total,new_cluster])\n",
    "        \n",
    "    \"\"\"\n",
    "    Just combine all the data points that were generated into a single vector\n",
    "    This is useful since we can later use these as binary indices to find out the \n",
    "    respective clusters\"\"\"\n",
    "\n",
    "    N, D = dataset_total.shape\n",
    "    \"\"\"\n",
    "        Note that N gives the number of samples in the dataset\n",
    "        D is the number of dimensions. Only 2D Case is visualisable\n",
    "        other n-D we can just use the data accordingly\"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "        Beginning of the Gaussian Mixture Model \n",
    "        We are going to be assuming that all points belong to the same cluster\n",
    "        initially. Then we shall, decompose the data into clusters or combine them\n",
    "        to have reduced clusters\"\"\"\n",
    "    multi_variate_normal = multivariate_normal # We are using the Higher Dimensional Gaussian\n",
    "    means = []  # This is the list that contains the means of the Gaussians that we fit for each cluster\n",
    "    sigma = np.eye(D) # This is the covariance matrix - for simplicity's sake we are setting the covariance matrix\n",
    "    prec = np.linalg.inv(sigma)  # Fixed precision matrix for all Gaussians\n",
    "    zs = np.zeros([N], dtype=int)  # Latent variables that need to be inferred\n",
    "    C = []  # This is a list of arrays, each of size number of datapoints. The arrays are binary and indicate\n",
    "            # whether a particular element belongs to a cluster or not. Note that each element belongs to one cluster,\n",
    "            # the cluster with its highest probability\n",
    "    samples_per_cluster = []  # Count of each cluster\n",
    "    start_mean = np.ones(D)\n",
    "    start_variance = np.eye(D)\n",
    "    prec0 = np.linalg.inv(np.eye(D))\n",
    "    G = multi_variate_normal(mean=start_mean, cov=start_variance)## Base Distribution\n",
    "    C.append(np.ones(N, dtype=int))\n",
    "    zs[:] = 0        ## Sets all the elements to 0 - assume that all points belonged to the same cluster\n",
    "    samples_per_cluster.append(N)   #Initially all points in same cluster\n",
    "    means.append(G.rvs())           # Add the mean of the starting cluster\n",
    "    K = 1                           # Number of Clusters is now, 1.\n",
    "    print(\"Gibbs Sampling Started....\\n\")\n",
    "    for iteration in range(20):     # Heuristic - Can be anything of the users choice.\n",
    "        \"\"\"Gibbs Sampling starts here\n",
    "           Note that we are simulating the Chinese Restaurant Process for our model\n",
    "           to cluster the data points\"\"\"\n",
    "        \n",
    "        for i in range(N):          # Gibbs Sampling Begin\n",
    "            \"\"\"\n",
    "                Unassign this particular point.\n",
    "                In each iteration, assign each point to a new \n",
    "                cluster with probability 1/(alpha+n) or add to an existing\n",
    "                cluster with probability proportional to the number of points in every\n",
    "                cluster, i.e., (n_k)/(n+alpha), where n_k is the number of points in cluster k\n",
    "                \"\"\"\n",
    "            \n",
    "            zi = zs[i]              \n",
    "            C[zi][i] = 0\n",
    "            samples_per_cluster[zi] -= 1\n",
    "            \"\"\"\n",
    "                Suppose after unassigning, number of clusters goes to zero.\n",
    "                \"\"\"\n",
    "            if samples_per_cluster[zi] == 0:\n",
    "                zs[zs > zi] -= 1\n",
    "                del C[zi]\n",
    "                del samples_per_cluster[zi]\n",
    "                del means[zi]\n",
    "                K -= 1\n",
    "            probs = np.zeros(K+1)\n",
    "            zs_minus_i = zs[np.arange(len(zs)) != i]\n",
    "            \"\"\"\n",
    "                This is the step where Gibbs Sampling Starts\n",
    "                We normalize the probability since we need to multiply the likelihood \n",
    "                as per max-likelihood principle\n",
    "            \"\"\"\n",
    "            for k in range(K):\n",
    "                nk_minus = zs_minus_i[zs_minus_i == k].shape[0]\n",
    "                crp = nk_minus / (N + alpha - 1)\n",
    "                probs[k] = crp * multi_variate_normal.pdf(dataset_total[i], means[k], sigma)\n",
    "            crp = alpha / (N + alpha - 1)\n",
    "            likelihood = multi_variate_normal.pdf(dataset_total[i], start_mean, start_variance+sigma)\n",
    "            probs[K] = crp*likelihood\n",
    "            probs /= np.sum(probs)\n",
    "            z = np.random.multinomial(n=1, pvals=probs).argmax()\n",
    "            if z == K:\n",
    "                C.append(np.zeros(N, dtype=int))\n",
    "                samples_per_cluster.append(0)\n",
    "                means.append(G.rvs())\n",
    "                K+=1\n",
    "            zs[i] = z\n",
    "            C[z][i] = 1\n",
    "            samples_per_cluster[z] += 1\n",
    "        \"\"\"\n",
    "            Sampling is done here.\n",
    "            Sampling from the conditional distribution\n",
    "        \"\"\"\n",
    "        for k in range(K):\n",
    "            Xk = dataset_total[zs == k]\n",
    "            samples_per_cluster[k] = Xk.shape[0]\n",
    "            lambda_post = prec0 + samples_per_cluster[k]*prec\n",
    "            cov_post = np.linalg.inv(lambda_post)\n",
    "            left = cov_post\n",
    "            right = prec0 @ start_mean + samples_per_cluster[k]*prec @ np.mean(Xk, axis=0)\n",
    "            means_post = left @ right\n",
    "            means[k] = multi_variate_normal.rvs(means_post, cov_post)\n",
    "        \"\"\"\n",
    "            Below we plot - by using the corresponding indices for each cluster.\n",
    "        \"\"\"\n",
    "    print(\"Gibbs Sampling Done! - Plotting now....\\n\")\n",
    "    km.fit(dataset_total)\n",
    "    predicted = km.predict(dataset_total)\n",
    "    Y=np.zeros(len(dataset_total))\n",
    "    for k in range(K):\n",
    "        Y[C[k]==1]=k+1\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=2)\n",
    "    fig.set_figheight(5)\n",
    "    fig.set_figwidth(15)\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.title('KMeans++ (k='+str(k_)+')')\n",
    "    plt.xlabel('X Axis')\n",
    "    plt.ylabel('Y Axis')\n",
    "    plt.scatter(dataset_total[:,0],dataset_total[:,1],c=predicted,cmap='plasma')\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.title('CRP Dirichlet Process - Found '+str(K) + ' after 20 iterations')\n",
    "    plt.scatter(dataset_total[:,0],dataset_total[:,1],c=Y,cmap='plasma')\n",
    "    plt.xlabel('X Axis')\n",
    "    plt.ylabel('Y Axis')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9e5a1b0b0c1458587b3572efcfc9b5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=5000.05, description='alpha', max=10000.0, min=0.1), IntSlider(value=3â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.chinese_restaurant_process_clustering(alpha, seed, k_, num_cluster)>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ipywidgets import interact\n",
    "interact(chinese_restaurant_process_clustering, alpha=(0.1,10000), seed=(1,5),num_cluster=(1,8),k_=(1,10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
